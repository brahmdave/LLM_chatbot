{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkQPUNXKG773FuF35Ep+3z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vb3gxz5AACt",
        "outputId": "b9a9b36b-61b7-4f28-9b15-54e6923da4c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.36.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import sqlite3\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "os.environ['API_KEY']=#gemini api\n",
        "# Configure Google Generative AI\n",
        "genai.configure(api_key=os.getenv('API_KEY'))\n",
        "model_1 = genai.GenerativeModel('gemini-1.5-flash')\n",
        "model = model_1.start_chat(history=[])\n",
        "# Create a database connection\n",
        "conn = sqlite3.connect(\"chat.db\", check_same_thread=False)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create tables to store users and chat messages if they do not exist\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS users (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    username TEXT UNIQUE,\n",
        "    password TEXT\n",
        ")\n",
        "\"\"\")\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS chats (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    user_id INTEGER,\n",
        "    message TEXT,\n",
        "    sender TEXT,\n",
        "    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
        "    FOREIGN KEY (user_id) REFERENCES users (id)\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "# Helper function to create a new user\n",
        "def create_user(username, password):\n",
        "    try:\n",
        "        cursor.execute(\"INSERT INTO users (username, password) VALUES (?, ?)\", (username, password))\n",
        "        conn.commit()\n",
        "    except sqlite3.IntegrityError:\n",
        "        st.error(\"Username already exists.\")\n",
        "\n",
        "# Helper function to authenticate a user\n",
        "def authenticate_user(username, password):\n",
        "    cursor.execute(\"SELECT id FROM users WHERE username = ? AND password = ?\", (username, password))\n",
        "    return cursor.fetchone()\n",
        "\n",
        "# Helper function to add a message to the chat history\n",
        "def add_message(user_id, message, sender):\n",
        "    cursor.execute(\"INSERT INTO chats (user_id, message, sender) VALUES (?, ?, ?)\", (user_id, message, sender))\n",
        "    conn.commit()\n",
        "\n",
        "# Helper function to fetch chat history\n",
        "def fetch_chat_history(user_id):\n",
        "    cursor.execute(\"SELECT message, sender FROM chats WHERE user_id = ? ORDER BY timestamp\", (user_id,))\n",
        "    return cursor.fetchall()\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Chat App with LLM Integration\")\n",
        "\n",
        "# State management for user session\n",
        "if 'logged_in' not in st.session_state:\n",
        "    st.session_state.logged_in = False\n",
        "    st.session_state.user_id = None\n",
        "    st.session_state.page = 'login'\n",
        "\n",
        "# Helper function to handle login\n",
        "def handle_login():\n",
        "    username = st.text_input(\"Username\", key=\"login_username\")\n",
        "    password = st.text_input(\"Password\", type=\"password\", key=\"login_password\")\n",
        "    if st.button(\"Login\"):\n",
        "        user = authenticate_user(username, password)\n",
        "        if user:\n",
        "            st.session_state.logged_in = True\n",
        "            st.session_state.user_id = user[0]\n",
        "            st.session_state.username = username\n",
        "            st.session_state.page = 'chat'\n",
        "            st.experimental_rerun()\n",
        "        else:\n",
        "            st.error(\"Invalid username or password.\")\n",
        "\n",
        "# Helper function to handle signup\n",
        "def handle_signup():\n",
        "    new_username = st.text_input(\"Username\", key=\"signup_username\")\n",
        "    new_password = st.text_input(\"Password\", type=\"password\", key=\"signup_password\")\n",
        "    if st.button(\"Signup\"):\n",
        "        if new_username and new_password:\n",
        "            create_user(new_username, new_password)\n",
        "            st.success(\"User created successfully! Please log in.\")\n",
        "        else:\n",
        "            st.error(\"Please enter both username and password.\")\n",
        "\n",
        "# Helper function to show chat interface\n",
        "def show_chat():\n",
        "    st.subheader(f\"Chat as {st.session_state.username}\")\n",
        "    user_input = st.text_area(\"Your message:\", key=\"user_input\")\n",
        "\n",
        "    if st.button(\"Send\"):\n",
        "        if user_input:\n",
        "            add_message(st.session_state.user_id, user_input, sender=\"user\")\n",
        "            # Generate a response from the LLM\n",
        "            response = model.send_message(user_input).text\n",
        "            add_message(st.session_state.user_id, response, sender=\"assistant\")\n",
        "            st.experimental_rerun()  # To update the chat history\n",
        "\n",
        "    # Display chat history for the logged-in user\n",
        "    chat_history = fetch_chat_history(st.session_state.user_id)\n",
        "    for message, sender in chat_history:\n",
        "        sender_name = \"You\" if sender == \"user\" else \"Assistant\"\n",
        "        st.write(f\"{sender_name}: {message}\")\n",
        "\n",
        "    # Logout option\n",
        "    if st.button(\"Logout\"):\n",
        "        st.session_state.logged_in = False\n",
        "        st.session_state.user_id = None\n",
        "        st.session_state.page = 'login'\n",
        "        st.experimental_rerun()\n",
        "\n",
        "# Display the appropriate interface based on the state\n",
        "if not st.session_state.logged_in:\n",
        "    option = st.selectbox(\"Login or Signup\", [\"Login\", \"Signup\"])\n",
        "    if option == \"Login\":\n",
        "        handle_login()\n",
        "    elif option == \"Signup\":\n",
        "        handle_signup()\n",
        "else:\n",
        "    show_chat()\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n53i3CQACyO",
        "outputId": "93b5d6b0-6791-453d-9934-7108a1dc9e26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGCrBmBYAaF_",
        "outputId": "9b2e0b9d-b485-40d2-d967-fd1b3851bdc2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.226.246.149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VtBbH19Aoyh",
        "outputId": "bca1d8bc-0765-4b04-d3ad-7b6d4fbe8a34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.226.246.149:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.086s\n",
            "your url is: https://fine-clocks-matter.loca.lt\n",
            "2024-07-04 13:16:38.752 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:16:57.382 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:17:10.448 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:17:18.865 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:21:26.984 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:21:51.123 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:57:55.393 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:59:34.279 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 13:59:59.131 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 14:01:49.863 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 14:02:15.772 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 14:02:50.633 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 14:03:12.397 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 14:03:25.827 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "2024-07-04 14:03:33.288 Please replace `st.experimental_rerun` with `st.rerun`.\n",
            "\n",
            "`st.experimental_rerun` will be removed after 2024-04-01.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Initialize the pipeline for text generation\n",
        "# def initialize_pipeline():\n",
        "#     model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#     model = AutoModelForCausalLM.from_pretrained(\n",
        "#         model_name,\n",
        "#         torch_dtype=torch.float32,\n",
        "#         trust_remote_code=True,\n",
        "#     )\n",
        "#     pipe = pipeline(\n",
        "#         \"text-generation\",\n",
        "#         model=model,\n",
        "#         tokenizer=tokenizer,\n",
        "#         device=0 if torch.cuda.is_available() else -1  # Use GPU if available, else CPU\n",
        "#     )\n",
        "#     return pipe"
      ],
      "metadata": {
        "id": "Sr4AZssbwoT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJeRor4_HXm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import streamlit as st\n",
        "# import sqlite3\n",
        "\n",
        "# # Create a database connection\n",
        "# conn = sqlite3.connect(\"chat.db\")\n",
        "# cursor = conn.cursor()\n",
        "\n",
        "# # Create a table to store the chat messages\n",
        "# cursor.execute(\"\"\"\n",
        "# CREATE TABLE IF NOT EXISTS chats (\n",
        "#     id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "#     user TEXT,\n",
        "#     message TEXT,\n",
        "#     timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "# )\n",
        "# \"\"\")\n",
        "# conn.commit()\n",
        "\n",
        "# # Define the Streamlit app\n",
        "# st.title(\"Simple Chat App\")\n",
        "\n",
        "# # Get the user's input\n",
        "# user_input = st.text_area(\"Your message:\", \"\")\n",
        "\n",
        "# # If the user has submitted a message, add it to the database\n",
        "# if st.button(\"Send\"):\n",
        "#     if user_input:\n",
        "#         cursor.execute(\"INSERT INTO chats (user, message) VALUES (?, ?)\", (\"user\", user_input))\n",
        "#         conn.commit()\n",
        "#         st.write(\"Message sent!\")\n",
        "\n",
        "# # Display the chat history\n",
        "# cursor.execute(\"SELECT user, message FROM chats ORDER BY timestamp\")\n",
        "# for row in cursor.fetchall():\n",
        "#     st.write(f\"{row[0]}: {row[1]}\")\n",
        "\n",
        "# # Close the database connection\n",
        "# conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import streamlit as st\n",
        "# import sqlite3\n",
        "# from datetime import datetime\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "# import torch\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "\n",
        "#     torch_dtype=\"auto\",\n",
        "#     trust_remote_code=True,\n",
        "# )\n",
        "# pipe = pipeline(\n",
        "#         \"text-generation\",\n",
        "#         model=model,\n",
        "#         tokenizer=tokenizer,\n",
        "#         device=0 )\n"
      ],
      "metadata": {
        "id": "0THoF58tJVll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import streamlit as st\n",
        "# import sqlite3\n",
        "# from datetime import datetime\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "# import torch\n",
        "\n",
        "\n",
        "# # Initialize the pipeline for text generation\n",
        "\n",
        "# model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#         model_name,\n",
        "#         torch_dtype=torch.float32,\n",
        "#         trust_remote_code=True\n",
        "#     )\n",
        "# pipe = pipeline(\n",
        "#         \"text-generation\",\n",
        "#         model=model,\n",
        "#         tokenizer=tokenizer,\n",
        "#         device=0 )"
      ],
      "metadata": {
        "id": "o9aNw7I5HM-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import pipeline\n",
        "\n",
        "# generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n"
      ],
      "metadata": {
        "id": "lV-fSTvvE9cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# input_text = \"can you tell difference between astrology and astronomy\"\n",
        "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# outputs = model.generate(input_ids)\n",
        "# print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "B6XiapgzPn5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall torch transformers\n",
        "# !pip install torch transformers\n"
      ],
      "metadata": {
        "id": "IFYuw-pMR42D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "# print(generator(\"Hello, I'm a transformer model\")[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "j_YGqLSLSU0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
      ],
      "metadata": {
        "id": "YLw100LpRk80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install accelerate\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "\n",
        "#     torch_dtype=\"auto\",\n",
        "#     trust_remote_code=True,\n",
        "# )"
      ],
      "metadata": {
        "id": "jTTzonecTg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "\n",
        "# messages = [\n",
        "#       {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
        "# ]\n",
        "\n",
        "\n",
        "\n",
        "# generation_args = {\n",
        "#     \"max_new_tokens\": 500,\n",
        "#     \"return_full_text\": False,\n",
        "#     \"temperature\": 0.0,\n",
        "#     \"do_sample\": False,\n",
        "# }\n",
        "\n",
        "# output = pipe(messages, **generation_args)\n",
        "# print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "7L8K7JzTRI07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# # Initialize model and tokenizer\n",
        "# model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# # Initialize the pipeline with the GPU device (0 for the first GPU)\n",
        "# pipe = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     device=0  # Use 0 to specify the first GPU\n",
        "# )\n",
        "\n",
        "# # Initial message and context\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}\n",
        "# ]\n",
        "\n",
        "# # Define generation arguments\n",
        "# generation_args = {\n",
        "#     \"max_new_tokens\": 500,\n",
        "#     \"return_full_text\": False,\n",
        "#     \"temperature\": 0.0,\n",
        "#     \"do_sample\": False,\n",
        "# }\n",
        "\n",
        "# # Function to maintain conversation context\n",
        "# def generate_response(pipe, messages, generation_args):\n",
        "#     # Format the input for the model\n",
        "#     formatted_input = [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages]\n",
        "\n",
        "#     # Generate text\n",
        "#     output = pipe(formatted_input, **generation_args)\n",
        "\n",
        "#     # Extract and return the generated text\n",
        "#     return output[0]['generated_text']\n",
        "\n",
        "# # Generate the first response\n",
        "# response = generate_response(pipe, messages, generation_args)\n",
        "# print(\"Model:\", response)\n",
        "\n",
        "# # Append the model's response to the conversation history\n",
        "# messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "# # Continue the conversation\n",
        "# # Example: Add more user input\n",
        "# new_user_input = \"Can you explain the solution?\"\n",
        "# messages.append({\"role\": \"user\", \"content\": new_user_input})\n",
        "\n",
        "# # Generate the next response\n",
        "# response = generate_response(pipe, messages, generation_args)\n",
        "# print(\"Model:\", response)\n",
        "\n",
        "# # Update the conversation history\n",
        "# messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "id": "Cg6wPBu2Wa0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "\n",
        "#     torch_dtype=\"auto\",\n",
        "#     trust_remote_code=True,  )\n"
      ],
      "metadata": {
        "id": "FdfzPTcZnWCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# pipe = pipeline(\n",
        "#         \"text-generation\",\n",
        "#         model=model,\n",
        "#         tokenizer=tokenizer,\n",
        "#         device=0  # Use GPU (0 for the first GPU) or -1 for CPU\n",
        "#     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdnF_HZKn0Gu",
        "outputId": "20820011-5462-446f-fc63-686549c72ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# # Save the pipe variable to a file in Google Drive\n",
        "# with open('/content/drive/MyDrive/LLM_chat/pipe.pkl', 'wb') as f:\n",
        "#     pickle.dump(pipe, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "JRzm11DeoW1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the pipe variable from Google Drive\n",
        "# with open('/content/drive/MyDrive/LLM_chat/pipe.pkl', 'rb') as f:\n",
        "#     pipe = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "BLXv_rRanRG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# # Function to initialize the pipeline\n",
        "# def initialize_pipeline(device=0):\n",
        "#   # Load the pipe variable from Google Drive\n",
        "\n",
        "#     model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#     model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "\n",
        "#     torch_dtype=\"auto\",\n",
        "#     trust_remote_code=True,  )\n",
        "\n",
        "\n",
        "#     pipe = pipeline(\n",
        "#         \"text-generation\",\n",
        "#         model=model,\n",
        "#         tokenizer=tokenizer,\n",
        "#         device=0  # Use GPU (0 for the first GPU) or -1 for CPU\n",
        "#     )\n",
        "#     return pipe\n",
        "\n",
        "# # Function to add user input to the conversation history\n",
        "# def add_user_input(messages, user_input):\n",
        "#     messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "#     return messages\n",
        "\n",
        "# # Function to generate a response from the model\n",
        "# def generate_response(pipe, messages, generation_args):\n",
        "#     formatted_input = [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages]\n",
        "#     output = pipe(formatted_input, **generation_args)\n",
        "#     generated_text = output[0]['generated_text']\n",
        "#     messages.append({\"role\": \"assistant\", \"content\": generated_text})\n",
        "#     return generated_text\n",
        "\n",
        "# # Function to print the conversation\n",
        "# def print_conversation(messages):\n",
        "#     for message in messages:\n",
        "#         role = \"User\" if message[\"role\"] == \"user\" else \"Assistant\"\n",
        "#         print(f\"{role}: {message['content']}\\n\")\n",
        "\n",
        "# # Main execution\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     # Initialize the pipeline\n",
        "#     pipe = initialize_pipeline()\n",
        "\n",
        "#     # Initial conversation history\n",
        "#     messages = []\n",
        "\n",
        "#     # Define generation arguments\n",
        "#     generation_args = {\n",
        "#         \"max_new_tokens\": 500,\n",
        "#         \"return_full_text\": False,\n",
        "#         \"temperature\": 0.0,\n",
        "#         \"do_sample\": False,\n",
        "#     }\n",
        "\n",
        "#     # First user input\n",
        "#     user_input = \"What about solving a 2x + 3 = 7 equation?\"\n",
        "#     messages = add_user_input(messages, user_input)\n",
        "\n",
        "#     # Generate and print response\n",
        "#     response = generate_response(pipe, messages, generation_args)\n",
        "#     print_conversation(messages)\n",
        "\n",
        "#     # Additional user input\n",
        "#     user_input = \"Can you explain the solution?\"\n",
        "#     messages = add_user_input(messages, user_input)\n",
        "\n",
        "#     # Generate and print response\n",
        "#     response = generate_response(pipe, messages, generation_args)\n",
        "#     print_conversation(messages)\n"
      ],
      "metadata": {
        "id": "ojqRJQh2Vq38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade openai"
      ],
      "metadata": {
        "id": "co-kFIMoS8UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import the OpenAI Python library for calling the OpenAI API\n",
        "# from openai import OpenAI\n",
        "\n",
        "# import os\n",
        "\n",
        "# client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"openapikey\"))"
      ],
      "metadata": {
        "id": "n31RqTULJlva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  vertexai.init(project=\"\", location=\"asia-south1\")\n",
        "#   model = GenerativeModel(\n",
        "#     \"gemini-1.5-flash-001\",\n",
        "#   )\n",
        "#   responses = model.generate_content(\n",
        "#       [],\n",
        "#       generation_config=generation_config,\n",
        "#       safety_settings=safety_settings,\n",
        "#       stream=True,\n",
        "#   )\n",
        "\n",
        "#   for response in responses:\n",
        "#     print(response.text, end=\"\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nMc11CyHX-JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import vertexai\n",
        "# from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "# # Authenticate to GCP (if you haven't already)\n",
        "# !gcloud auth application-default login\n"
      ],
      "metadata": {
        "id": "53KQVvgaS3ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation_config = {\n",
        "#     \"max_output_tokens\": 250,\n",
        "#     \"temperature\": 1,\n",
        "#     \"top_p\": 0.95,\n",
        "# }"
      ],
      "metadata": {
        "id": "SYTykT6JY6E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UgAxlt81df9C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nfp6Be7Idcdq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "\n",
        "# genai.configure(api_key=os.environ['API_KEY'])\n",
        "# model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "# chat = model.start_chat(history=[])\n",
        "\n",
        "# response = chat.send_message(\"Which is one of the best place to visit in India during summer?\")\n",
        "# print(response.text)\n",
        "# response = chat.send_message(\"Tell me more about that place in 50 words\")\n",
        "# print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "m6VdGlfeTK4O",
        "outputId": "b32239a2-22ae-48b7-a77c-b837d5e04f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's tough to pick just one \"best\" place, as India offers so much variety! It really depends on what you're looking for in a summer escape. \n",
            "\n",
            "**Here are some top contenders, each with their own unique appeal:**\n",
            "\n",
            "**For cool mountain air and breathtaking views:**\n",
            "\n",
            "* **Manali, Himachal Pradesh:** Known for its stunning landscapes, apple orchards, and adventure activities like trekking and paragliding.\n",
            "* **Darjeeling, West Bengal:** Famous for its tea plantations, toy train, and panoramic views of the Himalayas.\n",
            "* **Ladakh, Jammu & Kashmir:** A high-altitude desert with rugged mountains, ancient monasteries, and opportunities for trekking and motorcycle touring.\n",
            "* **Shimla, Himachal Pradesh:** A historic hill station with colonial architecture, bustling markets, and scenic walks.\n",
            "\n",
            "**For coastal breezes and beaches:**\n",
            "\n",
            "* **Goa:** A popular beach destination with sandy shores, vibrant nightlife, and Portuguese architecture.\n",
            "* **Kerala:** Known as \"God's Own Country,\" Kerala offers serene backwaters, lush greenery, and beautiful beaches like Kovalam and Varkala.\n",
            "* **Andaman and Nicobar Islands:** A tropical paradise with pristine beaches, coral reefs, and untouched islands.\n",
            "\n",
            "**For historical and cultural immersion:**\n",
            "\n",
            "* **Leh, Ladakh:** A gateway to the Tibetan Buddhist culture, with ancient monasteries, colorful prayer flags, and stunning landscapes.\n",
            "* **Varanasi, Uttar Pradesh:** A holy city on the banks of the Ganges, known for its ancient temples, ghats, and spiritual atmosphere.\n",
            "* **Jaipur, Rajasthan:** The \"Pink City,\" famed for its forts, palaces, and colorful markets.\n",
            "\n",
            "**For wildlife encounters:**\n",
            "\n",
            "* **Jim Corbett National Park, Uttarakhand:** Home to Bengal tigers, elephants, and other wildlife.\n",
            "* **Kaziranga National Park, Assam:** Known for its one-horned rhinoceros population.\n",
            "* **Gir National Park, Gujarat:** The only place in the world where you can see Asiatic lions in their natural habitat.\n",
            "\n",
            "**To help you choose the best place for you, please tell me:**\n",
            "\n",
            "* What kind of activities are you interested in (beach relaxation, hiking, sightseeing, adventure sports, etc.)?\n",
            "* What is your budget?\n",
            "* How long do you want to stay?\n",
            "\n",
            "Once I know your preferences, I can recommend the best place in India for your summer vacation! \n",
            "\n",
            "Okay, tell me which place you're interested in learning more about! I can give you 50 words on any of the options I listed, from the cool mountains of Manali to the vibrant beaches of Goa. Just let me know which one catches your eye!  \n",
            "\n"
          ]
        }
      ]
    }
  ]
}